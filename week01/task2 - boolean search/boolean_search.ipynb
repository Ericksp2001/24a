{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T00:34:25.239538Z",
     "start_time": "2024-05-01T00:34:07.885829Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar el corpus de stopwords y punkt si no está disponible\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T00:34:40.219312Z",
     "start_time": "2024-05-01T00:34:40.209307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_inverted_index(directory):\n",
    "    index = {}\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files[:100]:  # Take only the first 100 files\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().lower()  # Convert to lowercase\n",
    "                tokens = word_tokenize(content)\n",
    "                tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "                for word in tokens:\n",
    "                    if word in index:\n",
    "                        index[word].append(file_path)\n",
    "                    else:\n",
    "                        index[word] = [file_path]\n",
    "    return index"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:32:55.390506Z",
     "start_time": "2024-05-01T00:34:45.442315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Directory where the documents are located\n",
    "directory = \"../../data\"\n",
    "\n",
    "inverted_index = create_inverted_index(directory)\n",
    "print(\"Inverted index created.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:34:23.290549Z",
     "start_time": "2024-05-01T01:34:16.918372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the inverted index into a dataframe\n",
    "df_index = pd.DataFrame([(word, files) for word, files in inverted_index.items()], columns=['Word', 'Files'])\n",
    "\n",
    "# Save the inverted index as a CSV file\n",
    "df_index.to_csv('inverted_index.csv', index=False)\n",
    "\n",
    "print(\"Inverted index saved as 'inverted_index.csv'.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved as 'inverted_index.csv'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:59:18.015138Z",
     "start_time": "2024-05-01T01:59:14.661824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar el archivo CSV generado\n",
    "df_index = pd.read_csv('inverted_index.csv')\n",
    "\n",
    "# Mostrar el DataFrame cargado\n",
    "print(df_index.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word                                              Files\n",
      "0       ﻿the  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "1    project  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "2  gutenberg  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "3      ebook  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "4    complet  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:53:01.223395Z",
     "start_time": "2024-05-01T01:53:01.212336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_query(query, inverted_index):\n",
    "    query_parts = query.split()\n",
    "    result = None\n",
    "    \n",
    "    while query_parts:\n",
    "        token = query_parts.pop(0)\n",
    "        if token == 'AND':\n",
    "            operand1 = result\n",
    "            operand2 = set(inverted_index[query_parts.pop(0)])\n",
    "            result = operand1.intersection(operand2)\n",
    "        elif token == 'OR':\n",
    "            operand1 = result\n",
    "            operand2 = set(inverted_index[query_parts.pop(0)])\n",
    "            result = operand1.union(operand2)\n",
    "        elif token == 'NOT':\n",
    "            operand1 = result\n",
    "            operand2 = set(inverted_index[query_parts.pop(0)])\n",
    "            result = operand1.difference(operand2)\n",
    "        else:\n",
    "            result = set(inverted_index[token])\n",
    "    \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:02:13.691021Z",
     "start_time": "2024-05-01T02:02:13.681198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"hi AND bye\"\n",
    "result = process_query(query, inverted_index)\n",
    "\n",
    "# Formatear y mostrar resultados de manera más elegante\n",
    "formatted_results = [os.path.basename(file_path) for file_path in result]\n",
    "print(\"Documentos encontrados:\")\n",
    "for file_name in formatted_results:\n",
    "    print(file_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados:\n",
      "pg4300.txt\n",
      "pg2160.txt\n",
      "pg39407.txt\n",
      "pg98.txt\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
