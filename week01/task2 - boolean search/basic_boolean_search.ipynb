{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Boolean Search in Documents\n",
    "\n",
    "## Objective\n",
    "Expand the simple term search functionality to include Boolean search capabilities. This will allow users to perform more complex queries by combining multiple search terms using Boolean operators.\n",
    "\n",
    "## Problem Description\n",
    "You must enhance the existing search engine from the previous exercise to support Boolean operators: AND, OR, and NOT. This will enable the retrieval of documents based on the logical relationships between multiple terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Step 1: Update Data Preparation\n",
    "Ensure that the documents are still loaded and preprocessed from the previous task. The data should be clean and ready for advanced querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T03:20:15.031967Z",
     "start_time": "2024-05-01T03:20:11.200484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar el corpus de stopwords y punkt si no está disponible\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an Inverted Index\n",
    "\n",
    "Create an inverted index from the documents. This index maps each word to the set of document IDs in which that word appears. This facilitates word lookup in the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T00:34:40.219312Z",
     "start_time": "2024-05-01T00:34:40.209307Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(directory):\n",
    "    index = {}\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files[:100]:  # Take only the first 100 files\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().lower()  # Convert to lowercase\n",
    "                tokens = word_tokenize(content)\n",
    "                tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "                for word in tokens:\n",
    "                    if word in index:\n",
    "                        index[word].append(file_path)\n",
    "                    else:\n",
    "                        index[word] = [file_path]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:32:55.390506Z",
     "start_time": "2024-05-01T00:34:45.442315Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Directory where the documents are located\u001b[39;00m\n\u001b[0;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m inverted_index \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_inverted_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInverted index created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36mcreate_inverted_index\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      9\u001b[0m content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(content)\n\u001b[1;32m---> 11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m index:\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(content)\n\u001b[1;32m---> 11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m index:\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:219\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m--> 219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\data.py:1055\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;124;03m    Read up to ``size`` bytes, decode them using this reader's\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    encoding, and return the resulting unicode string.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03m    :rtype: unicode\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1055\u001b[0m     chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;66;03m# If linebuffer is not empty, then include it in the result\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinebuffer:\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\data.py:1338\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader._read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;66;03m# Read the requested number of bytes.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1338\u001b[0m     new_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1340\u001b[0m     new_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mread(size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Directory where the documents are located\n",
    "directory = \"../../data\"\n",
    "\n",
    "inverted_index = create_inverted_index(directory)\n",
    "print(\"Inverted index created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:34:23.290549Z",
     "start_time": "2024-05-01T01:34:16.918372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved as 'inverted_index.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Convert the inverted index into a dataframe\n",
    "df_index = pd.DataFrame([(word, files) for word, files in inverted_index.items()], columns=['Word', 'Files'])\n",
    "\n",
    "# Save the inverted index as a CSV file\n",
    "df_index.to_csv('inverted_index.csv', index=False)\n",
    "\n",
    "print(\"Inverted index saved as 'inverted_index.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Query Processing\n",
    "- **Parse the Query**: Implement a function to parse the input query to identify the terms and operators.\n",
    "- **Search Documents**: Based on the parsed query, implement the logic to retrieve and rank the documents according to the Boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T01:59:18.015138Z",
     "start_time": "2024-05-01T01:59:14.661824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word                                              Files\n",
      "0       ﻿the  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "1    project  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "2  gutenberg  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "3      ebook  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n",
      "4    complet  ['../../data\\\\pg100.txt', '../../data\\\\pg100.t...\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV generado\n",
    "df_index = pd.read_csv('inverted_index.csv')\n",
    "\n",
    "# Mostrar el DataFrame cargado\n",
    "print(df_index.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T02:44:20.498433Z",
     "start_time": "2024-05-01T02:44:20.480929Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_query(query, df_index):\n",
    "    result = df_index[df_index['Word'] == query]['Files'].tolist()\n",
    "    result = eval(result[0]) if result else []\n",
    "    return set(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Displaying Results\n",
    "- **Output the Results**: Display the documents that match the query criteria. Include functionalities to handle queries that result in no matching documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados:\n",
      "pg4085.txt\n",
      "pg4300.txt\n",
      "pg67979.txt\n",
      "pg74.txt\n",
      "pg64317.txt\n",
      "pg76.txt\n",
      "pg16594.txt\n",
      "pg28054.txt\n",
      "pg10681.txt\n",
      "pg514.txt\n",
      "pg205.txt\n",
      "pg1400.txt\n",
      "pg2160.txt\n",
      "pg19926.txt\n",
      "pg996.txt\n",
      "pg55.txt\n",
      "pg100.txt\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta\n",
    "query = \"bug\"\n",
    "\n",
    "# Realizar la búsqueda\n",
    "result = search_query(query, df_index)\n",
    "\n",
    "# Formatear y mostrar los resultados de manera elegante\n",
    "if result:\n",
    "    print(\"Documentos encontrados:\")\n",
    "    formatted_results = [os.path.basename(file_path) for file_path in result]\n",
    "    for file_name in formatted_results:\n",
    "        print(file_name)\n",
    "else:\n",
    "    print(\"No se encontraron documentos relacionados con la consulta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "- **Correctness**: The Boolean search implementation should correctly interpret and process the queries according to the Boolean logic.\n",
    "- **Efficiency**: Consider the efficiency of your search process, especially as the complexity of queries increases.\n",
    "- **User Experience**: Ensure that the interface for inputting queries and viewing results is user-friendly.\n",
    "\n",
    "This exercise will deepen your understanding of how search engines process and respond to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
